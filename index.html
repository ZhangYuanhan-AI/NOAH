<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
  	<title>OmniBenchmark</title>
      <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
          if you update and want to force Facebook to re-scrape. -->
  	<meta property="og:image" content="./resources/teaser-min.png"/>
  	<meta property="og:title" content="Benchmarking Omni-Vision Representation through the Lens of Visual Realms." />
  	<meta property="og:description" content="A new computer vision benchmark and a new super contrastive learning framework." />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="summary" />
    <meta property="twitter:title"         content="Benchmarking Omni-Vision Representation through the Lens of Visual Realms." />
    <meta property="twitter:description"   content="A new computer vision benchmark and a new super contrastive learning framework." />
    <meta property="twitter:image"         content="./resources/teaser-min.png" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Add your Google Analytics tag here -->
    <!-- <script async
            src="https://www.googletagmanager.com/gtag/js?id=UA-97476543-1"></script> -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>

</head>

<body>
<div class="container">
    <div class="title">
        Benchmarking Omni-Vision Representation   <br> 
        through the Lens of Visual Realms
    </div>

    <!-- <div class="venue">
        arXiv
    </div> -->

    <br><br>

    <div class="author">
        <a href="https://davidzhangyuanhan.github.io/">Yuanhan Zhang</a><sup>1</sup>
    </div>

    <div class="author">
        <a href="https://scholar.google.com.hk/citations?user=ngPR1dIAAAAJ&hl=zh-CN">Zhenfei Yin</a><sup>2</sup>
    </div>

    <br>

    <div class="author">
        <a href="https://scholar.google.com.hk/citations?user=VU5ObUwAAAAJ&hl=zh-CN">Jing Shao</a><sup>2</sup>
    </div>
    <div class="author">
        <a href="https://liuziwei7.github.io/">Ziwei liu</a><sup>1</sup>
    </div>

    <br><br>

    <div class="affiliation"><sup>1&nbsp;</sup>S-Lab, Nanyang Technological University</div>
    <div class="affiliation"><sup>2&nbsp;</sup>SenseTime Reseach</div>

    <br><br>

    <div class="links"><a href="">[Paper]</a></div>
    <!-- <div class="links"><a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ">[Video]</a></div> -->
    <div class="links"><a href="https://github.com/Davidzhangyuanhan/OmniBenchmark">[Code]</a></div>

    <br><br>

    <img style="width: 80%;" src="./resources/teaser-min.png" alt="Teaser figure."/>
    <!-- <br>
    <p style="width: 80%;">
        This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful project</a>, and inherits the modifications made by <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a>.
        The code can be found <a href="https://github.com/elliottwu/webpage-template">here</a>.
    </p> -->

    <br><br>
    <hr>

    <h1>TL;DR</h1>
    <h2>New Benchmark</h2>
    <p style="width: 80%;">
        Omni-Realm Benchmark (OmniBenchmark) is a <strong>diverse</strong> (21 semantic realm-wise datasets) and <strong>concise</strong> (realm-wise datasets have no concepts overlapping) benchmark for evaluating pre-trained model generalization across semantic super-concepts/realms, e.g. across mammals to aircrafts.
    </p>
    <img style="width: 80%;" src="./resources/Omnibenchmark_annimation.gif" alt="omnibenchmark annimation."/>
    <br><br>
    <h2>New Supervised Contrastive Learning Framework</h2>
    <p style="width: 80%;">
        We introduces a new supervised contrastive learning framework, namely Relational Contrastive (ReCo) learning, that aims to be better suited for omni-vision representation.
    </p>
    <img style="width: 80%;" src="./resources/ReCo_annimation.gif" alt="reCo annimation."/>

    <br><br>
    <hr>


    <h1>Abstract</h1>
    <p style="width: 80%;">
        Though impressive performance has been achieved in specific visual realms (\eg faces, dogs, and places), an omni-vision representation generalizing to many natural visual domains is highly desirable.
        But, existing benchmarks are biased and inefficient to evaluate the omni-vision representation: either they can only evaluate specific realms, or they cover more realms at the expense of including more datasets that have concepts overlapping, resulting in repetitively evaluating several realms.
        In this paper, we propose Omni-Realm Benchmark (<strong>OmniBenchmark</strong>). It includes 21 realm-wise datasets with 7,372 concepts and 1,074,346 images. These realm-wise datasets cover most visual realms in the world and have no concept overlapping. 
        With OmniBenchmark, we diagnose the advances in omni-vision representation studies in architectures (from CNNs to transformers) and in learning paradigms (from supervised learning to self-supervised learning).
        We reveal multiple practical observations to facilitate future research.
        Beyond that, we propose a new supervised contrastive learning framework, namely <strong>Re</strong>lational <strong>Co</strong>ntrastive learning (<strong>ReCo</strong>), for a better omni-vision representation.
        ReCo pulls two classes belonging to the same visual realm closer instead of pushing them apart, learning the semantic relation between classes. Consequently, ReCo achieves superior performance on both ImageNet and OmniBenchmark.
    </p>

    <br><br>
    <hr>

    <!-- <h1>Video</h1>
    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameBorder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div>

    <br><br>
    <hr> -->

    <!-- <h1>Method Overview</h1>
    <img style="width: 80%;" src="./resources/method.jpg"
         alt="Method overview figure"/>
    <br>
    <a class="links" href="https://github.com/elliottwu/webpage-template">[Code]</a>

    <br><br>
    <hr> -->

    <!-- <h1>Results</h1>
    <img style="width: 80%;" src="./resources/results.jpg"
         alt="Results figure"/>

    <br><br>
    <hr> -->

    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://arxiv.org">
            <img class="layered-paper-big" width="100%" src="./resources/paper.jpg" alt="Paper thumbnail"/>
        </a>
    </div>
    <div class="paper-info">
        <h3>Benchmarking Omni-Vision Representation through the Lens of Visual Realms</h3>
        <p>Yuanhan Zhang, Zhenfei Yin, Jing Shao and Ziwei liu</p>
        <p>In Conference, 20XX.</p>
        <pre><code>@InProceedings{author20XXtitle,
    title = {Creative and Descriptive Paper Title},
    author = {Author, First and Author, Second and Author, Third and Author, Fourth and Author, Fifth},
    booktitle = {Conference},
    year = {20XX},
}</code></pre>
    </div>

    <br><br>
    <hr>

    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        TBA.
    </p>

    <br><br>
</div>

</body>

</html>
